# QueryRefinerRAG.py
import sqlite3
import numpy as np
import os
from typing import List, Dict, Optional
import openai
import json

# ã„ã¤ã§ã‚‚ã“ã®IDã ã‘ã‚’è¦‹ã‚‹
TARGET_USER_ID = "21639740"


class RAGSearchSystem:
    def __init__(self, main_db_path: str = "data/ncv_monitor.db", vector_db_path: str = "data/vectors.db"):
        self.main_db_path = main_db_path
        self.vector_db_path = vector_db_path

        # DBå­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not os.path.exists(self.vector_db_path):
            print(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«DBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.vector_db_path}")
            print("å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

        if not os.path.exists(self.main_db_path):
            print(f"âš ï¸ ãƒ¡ã‚¤ãƒ³DBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.main_db_path}")

        # APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
        config_path = os.path.join(os.path.dirname(__file__), "config", "ncv_special_config.json")
        api_key: Optional[str] = None
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    cfg = json.load(f)
                api_key = cfg.get('api_settings', {}).get('openai_api_key', '')
            except Exception:
                api_key = None

        if not api_key:
            api_key = os.getenv('OPENAI_API_KEY')

        if not api_key:
            raise RuntimeError("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.client = openai.OpenAI(api_key=api_key, timeout=20.0)

    # --- å‰å‡¦ç†ï¼ˆè³ªå•æ•´å½¢ã®ã¿ï¼šmini 1å›ï¼‰ ---
    def preprocess_question(self, question: str) -> str:
        """
        å…¥åŠ›ã®è³ªå•æ–‡ã‚’ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å‘ã‘ã®çŸ­ã„æ¤œç´¢èªã«æ•´å½¢ã—ã¦è¿”ã™ã€‚
        å¯èƒ½ãªé™ã‚Šåè©ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸­å¿ƒã€10ã€œ20æ–‡å­—ç¨‹åº¦ã‚’ç›®å®‰ã€‚
        """
        system_prompt = (
            "ã‚ãªãŸã¯æ¤œç´¢ã‚¯ã‚¨ãƒªå¤‰æ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚"
            "å…¥åŠ›ã•ã‚ŒãŸè³ªå•æ–‡ã‚’ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«é©ã—ãŸçŸ­ã„æ–‡ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚"
            "ãƒ«ãƒ¼ãƒ«:"
            "1. ç–‘å•è©ã‚„åŠ©è©ã‚’å‰Šé™¤ã—ã¦ã‚ˆã„ãŒã€æ„å‘³ä¸Šã®ä¸»èªãƒ»å¯¾è±¡ãƒ»è¡Œç‚ºã¯å¿…ãšæ®‹ã™ã€‚"
            "2. å›ºæœ‰åè©ã¯çœç•¥ã›ãšä¿æŒã™ã‚‹ã€‚"
            "3. æ›–æ˜§ãªäººç‰©å‚ç…§ï¼ˆã“ã®äººã€ã“ã„ã¤ç­‰ï¼‰ã¯ user_id=21639740 ã«ç½®ãæ›ãˆã‚‹ã€‚"
            "4. å‡ºåŠ›ã¯ä¸€æ–‡ã®ã¿ã§ã€ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã€‚"
            "5. å‡ºåŠ›å½¢å¼ã¯è‡ªç„¶ãªçŸ­æ–‡ã§ã‚ˆã„ï¼ˆå˜èªã®ç¾…åˆ—ã¯ç¦æ­¢ï¼‰ã€‚"
        )
        try:
            resp = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                max_tokens=50,
                temperature=0
            )
            refined = (resp.choices[0].message.content or "").strip()
            if not refined:
                refined = question
        except Exception:
            refined = question

        print(f"ğŸ§­ è³ªå•æ•´å½¢: {question} â†’ {refined}")
        return refined

    # --- ãƒ™ã‚¯ãƒˆãƒ«åŒ– ---
    def _get_embedding(self, text: str) -> np.ndarray:
        resp = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(resp.data[0].embedding, dtype=np.float32)

    # --- é¡ä¼¼ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢ï¼ˆå¸¸ã« TARGET_USER_ID ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰ ---
    def _search_similar_comments(self, query_vector: np.ndarray, top_k: int) -> List[Dict]:
        results: List[Dict] = []
        try:
            with sqlite3.connect(self.vector_db_path) as vconn:
                cur = vconn.cursor()
                cur.execute("""
                    SELECT cv.comment_id, cv.user_id, cv.comment_text, cv.vector_data, cv.broadcast_id
                    FROM comment_vectors cv
                    WHERE cv.user_id = ?
                """, (TARGET_USER_ID,))
                rows = cur.fetchall()

            for comment_id, uid, comment_text, vector_blob, broadcast_id in rows:
                stored = np.frombuffer(vector_blob, dtype=np.float32)
                sim = self._cosine_similarity(query_vector, stored)
                results.append({
                    "comment_id": comment_id,
                    "user_id": uid,
                    "comment_text": comment_text,
                    "broadcast_id": broadcast_id,
                    "similarity": sim,
                })

            results.sort(key=lambda x: x["similarity"], reverse=True)
            results = results[:top_k]

            # è¿½åŠ æƒ…å ±ä»˜ä¸ï¼ˆåå‰ãƒ»æ ã‚¿ã‚¤ãƒˆãƒ«ç­‰ï¼‰
            results = self._enrich_comment_results(results)

            print(f"ğŸ’¬ é¡ä¼¼ã‚³ãƒ¡ãƒ³ãƒˆ: {len(results)}ä»¶ (user_id={TARGET_USER_ID})")
            return results

        except Exception as e:
            print(f"âŒ ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    # --- é¡ä¼¼AIåˆ†ææ¤œç´¢ï¼ˆå¸¸ã« TARGET_USER_ID ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰ ---
    def _search_similar_analyses(self, query_vector: np.ndarray, top_k: int) -> List[Dict]:
        results: List[Dict] = []
        try:
            with sqlite3.connect(self.vector_db_path) as vconn:
                cur = vconn.cursor()
                cur.execute("""
                    SELECT av.analysis_id, av.user_id, av.analysis_text, av.vector_data, av.broadcast_id
                    FROM analysis_vectors av
                    WHERE av.user_id = ?
                """, (TARGET_USER_ID,))
                rows = cur.fetchall()

            for analysis_id, uid, analysis_text, vector_blob, broadcast_id in rows:
                stored = np.frombuffer(vector_blob, dtype=np.float32)
                sim = self._cosine_similarity(query_vector, stored)
                results.append({
                    "analysis_id": analysis_id,
                    "user_id": uid,
                    "analysis_text": analysis_text,
                    "broadcast_id": broadcast_id,
                    "similarity": sim,
                })

            results.sort(key=lambda x: x["similarity"], reverse=True)
            results = results[:top_k]

            # è¿½åŠ æƒ…å ±ä»˜ä¸
            results = self._enrich_analysis_results(results)

            print(f"ğŸ¤– é¡ä¼¼AIåˆ†æ: {len(results)}ä»¶ (user_id={TARGET_USER_ID})")
            return results

        except Exception as e:
            print(f"âŒ AIåˆ†ææ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    # --- è¿½åŠ æƒ…å ±ä»˜ä¸ï¼šã‚³ãƒ¡ãƒ³ãƒˆ ---
    def _enrich_comment_results(self, items: List[Dict]) -> List[Dict]:
        if not items:
            return items
        try:
            ids = [i["comment_id"] for i in items]
            placeholders = ",".join("?" for _ in ids)
            with sqlite3.connect(self.main_db_path) as conn:
                cur = conn.cursor()
                cur.execute(f"""
                    SELECT c.id, c.user_name, c.timestamp, c.elapsed_time,
                           b.lv_value, b.live_title, b.start_time,
                           su.display_name
                    FROM comments c
                    JOIN broadcasts b ON c.broadcast_id = b.id
                    LEFT JOIN special_users su ON c.user_id = su.user_id
                    WHERE c.id IN ({placeholders})
                """, ids)
                meta = {row[0]: {
                    "user_name": row[1],
                    "timestamp": row[2],
                    "elapsed_time": row[3],
                    "lv_value": row[4],
                    "live_title": row[5],
                    "start_time": row[6],
                    "display_name": row[7]
                } for row in cur.fetchall()}

            for it in items:
                info = meta.get(it["comment_id"], {})
                it.update(info)
            return items
        except Exception as e:
            print(f"âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆä»˜éšæƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
            return items

    # --- è¿½åŠ æƒ…å ±ä»˜ä¸ï¼šAIåˆ†æ ---
    def _enrich_analysis_results(self, items: List[Dict]) -> List[Dict]:
        if not items:
            return items
        try:
            ids = [i["analysis_id"] for i in items]
            placeholders = ",".join("?" for _ in ids)
            with sqlite3.connect(self.main_db_path) as conn:
                cur = conn.cursor()
                cur.execute(f"""
                    SELECT a.id, a.model_used, a.comment_count, a.analysis_date,
                           b.lv_value, b.live_title, b.start_time,
                           su.display_name
                    FROM ai_analyses a
                    JOIN broadcasts b ON a.broadcast_id = b.id
                    LEFT JOIN special_users su ON a.user_id = su.user_id
                    WHERE a.id IN ({placeholders})
                """, ids)
                meta = {row[0]: {
                    "model_used": row[1],
                    "comment_count": row[2],
                    "analysis_date": row[3],
                    "lv_value": row[4],
                    "live_title": row[5],
                    "start_time": row[6],
                    "display_name": row[7],
                } for row in cur.fetchall()}

            for it in items:
                info = meta.get(it["analysis_id"], {})
                it.update(info)
            return items
        except Exception as e:
            print(f"âš ï¸ åˆ†æä»˜éšæƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
            return items

    # --- é¡ä¼¼åº¦ ---
    def _cosine_similarity(self, v1: np.ndarray, v2: np.ndarray) -> float:
        dot = float(np.dot(v1, v2))
        n1 = float(np.linalg.norm(v1))
        n2 = float(np.linalg.norm(v2))
        return dot / (n1 * n2) if n1 and n2 else 0.0

    # --- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ ---
    def _build_context(self, comments: List[Dict], analyses: List[Dict]) -> str:
        parts: List[str] = []

        if comments:
            parts.append("ã€é–¢é€£ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã€‘")
            for i, c in enumerate(comments, 1):
                disp = (c.get("display_name") or c.get("user_name") or "").strip()
                if disp.startswith("ãƒ¦ãƒ¼ã‚¶ãƒ¼") and disp.replace("ãƒ¦ãƒ¼ã‚¶ãƒ¼", "").isdigit():
                    # ãƒ‹ã‚³ç”Ÿã®æ±ç”¨åã¯ user_name ã‚’å„ªå…ˆ
                    disp = (c.get("user_name") or disp).strip()

                parts.append(
                    f"{i}. ãƒ¦ãƒ¼ã‚¶ãƒ¼: {disp or 'ä¸æ˜'} (ID: {c.get('user_id')})"
                    f"\n   ã‚³ãƒ¡ãƒ³ãƒˆ: ã€Œ{c.get('comment_text','')}ã€"
                    f"\n   é…ä¿¡: {c.get('live_title','ä¸æ˜ãªé…ä¿¡')} / çµŒé: {c.get('elapsed_time','?')} [é¡ä¼¼åº¦: {c.get('similarity',0):.3f}]"
                )

        if analyses:
            parts.append("\nã€é–¢é€£ã™ã‚‹AIåˆ†æã€‘")
            for i, a in enumerate(analyses, 1):
                disp = (a.get("display_name") or "").strip()
                preview = (a.get("analysis_text") or "")[:150] + "..."
                parts.append(
                    f"{i}. ãƒ¦ãƒ¼ã‚¶ãƒ¼: {disp or 'ä¸æ˜'} (ID: {a.get('user_id')}) ã®åˆ†æ"
                    f"\n   ãƒ¢ãƒ‡ãƒ«: {a.get('model_used','ä¸æ˜')} / ä»¶æ•°: {a.get('comment_count','?')} / æ—¥ä»˜: {a.get('analysis_date','?')}"
                    f"\n   è¦ç´„: {preview}"
                    f"\n   é…ä¿¡: {a.get('live_title','ä¸æ˜ãªé…ä¿¡')} [é¡ä¼¼åº¦: {a.get('similarity',0):.3f}]"
                )

        return "\n\n".join(parts)

    # --- å›ç­”ç”Ÿæˆ ---
    def _generate_answer(self, question: str, context: str) -> str:
        system_prompt = (
            "ã‚ãªãŸã¯ãƒ‹ã‚³ãƒ‹ã‚³ç”Ÿæ”¾é€ã®åˆ†ææ‹…å½“ã§ã™ã€‚"
            "ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãƒ»åˆ†æï¼‰ã«ã®ã¿åŸºã¥ã„ã¦ã€ç°¡æ½”ã‹ã¤æ ¹æ‹ ä»˜ãã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
            "å¿…ãš user_id ã‚’æ˜è¨˜ã—ã€å¼•ç”¨ç®‡æ‰€ã‚’æ ¹æ‹ ã¨ã—ã¦ç¤ºã—ã¦ãã ã•ã„ã€‚"
            "ä¸è¶³ãŒã‚ã‚‹å ´åˆã¯ã€ä¸è¶³ã—ã¦ã„ã‚‹ã€ã¨è¿°ã¹ã¦ãã ã•ã„ã€‚"
        )
        user_prompt = f"è³ªå•: {question}\n\nå‚è€ƒæƒ…å ±:\n{context}\n\nã“ã®æƒ…å ±ã ã‘ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"

        try:
            resp = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=800,
                temperature=0.4
            )
            return (resp.choices[0].message.content or "").strip()
        except Exception as e:
            return f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"

    # --- ãƒ¡ã‚¤ãƒ³ ---
    def search_and_answer(self, question: str, top_k: int = 10) -> str:
        print(f"ğŸ” è³ªå•: {question}")
        print(f"ğŸ§­ å›ºå®š user_id={TARGET_USER_ID} ã‚’ä½¿ç”¨")

        # 1) è³ªå•æ•´å½¢ â†’ 2) ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        refined = self.preprocess_question(question)
        query_vec = self._get_embedding(refined)

        # 3) æ¤œç´¢ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã®ã¿10ä»¶ï¼‰
        comments = self._search_similar_comments(query_vec, top_k)
        analyses = []  # AIåˆ†æã¯ä½¿ç”¨ã—ãªã„

        # 4) å›ç­”ç”Ÿæˆ
        context = self._build_context(comments, analyses)
        if not context.strip():
            print("ğŸ§­ æ¤œç´¢çµæœ: 0ä»¶")
            return "ğŸ¤· é–¢é€£æƒ…å ±ãªã—"

        print(f"ğŸ“Š æ¤œç´¢çµæœ: ã‚³ãƒ¡ãƒ³ãƒˆ{len(comments)}ä»¶")
        return self._generate_answer(question, context)


if __name__ == "__main__":
    import sys
    rag = RAGSearchSystem()

    if len(sys.argv) > 1:
        question = " ".join(sys.argv[1:])
    else:
        question = "ã“ã®äººãªã«ãŒå¥½ãï¼Ÿ"

    ans = rag.search_and_answer(question)
    print(f"\nğŸ’¡ å›ç­”:\n{ans}")
