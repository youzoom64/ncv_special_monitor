#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦JSONå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
import re
from datetime import datetime
from bs4 import BeautifulSoup
from typing import Dict, List, Optional

def parse_html_to_json(html_file_path: str, output_dir: str = "converted_data"):
    """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦JSONå½¢å¼ã«å¤‰æ›"""
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # HTMLãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(html_file_path, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ï¼‰
    title = soup.find('title')
    user_name = "ãƒãƒ³ã‚«ã‚¹ã‚¢ãƒ‹ãƒ¡è±šå¤ªéƒ"  # HTMLã‹ã‚‰åˆ¤æ–­
    user_id = "21639740"  # HTMLã‹ã‚‰åˆ¤æ–­
    
    # å„é…ä¿¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    link_items = soup.find_all('div', class_='link-item')
    
    converted_count = 0
    
    for item in link_items:
        try:
            # é–‹å§‹æ™‚é–“ã‚’å–å¾—
            start_time_text = None
            for p in item.find_all('p'):
                if 'é–‹å§‹æ™‚é–“:' in p.text:
                    start_time_text = p.text.replace('é–‹å§‹æ™‚é–“:', '').strip()
                    break
            
            if not start_time_text:
                continue
            
            # ãƒªãƒ³ã‚¯ã‹ã‚‰lvç•ªå·ã‚’å–å¾—
            link = item.find('a')
            if not link or not link.get('href'):
                continue
            
            href = link.get('href')
            lv_match = re.search(r'(\d+)_21639740_comment\.html', href)
            if not lv_match:
                continue
            
            lv_number = lv_match.group(1)
            lv_value = f"lv{lv_number}"
            
            # é…ä¿¡ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ï¼ˆãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ï¼‰
            link_text = link.text.strip()
            # "ã€œã«ãŠã‘ã‚‹ã€œã®ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ"ã®éƒ¨åˆ†ã‹ã‚‰é…ä¿¡ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
            title_match = re.search(r'^([^:]+):', link_text)
            live_title = title_match.group(1) if title_match else "é…ä¿¡"
            
            # ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
            chat_data_div = item.find('div', class_='chat-data')
            if not chat_data_div:
                continue
            
            table = chat_data_div.find('table')
            if not table:
                continue
            
            # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            comments = []
            rows = table.find_all('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 4:
                    comment_no = int(cells[0].text.strip())
                    elapsed_time = cells[1].text.strip()
                    date_str = cells[2].text.strip()
                    comment_text = cells[3].find('b').text.strip()
                    
                    # æ—¥æ™‚ã‚’Unixã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›
                    try:
                        dt = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
                        timestamp = int(dt.timestamp())
                    except:
                        timestamp = 0
                    
                    comments.append({
                        "no": comment_no,
                        "date": timestamp,
                        "text": comment_text,
                        "elapsed_time": elapsed_time
                    })
            
            if not comments:
                continue
            
            # é–‹å§‹æ™‚é–“ã‚’Unixã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›
            try:
                start_dt = datetime.strptime(start_time_text, '%Y-%m-%d %H:%M:%S')
                start_timestamp = int(start_dt.timestamp())
            except:
                start_timestamp = 0
            
            # JSONå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            json_data = {
                "broadcast_info": {
                    "lv_value": lv_value,
                    "start_time": start_timestamp,
                    "live_title": live_title
                },
                "user_info": {
                    "user_id": user_id,
                    "user_name": user_name
                },
                "total_count": len(comments),
                "comments": comments
            }
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            output_file = os.path.join(output_dir, f"{lv_value}_comments.json")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å¤‰æ›å®Œäº†: {lv_value} ({live_title}) - {len(comments)}ã‚³ãƒ¡ãƒ³ãƒˆ")
            converted_count += 1
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            continue
    
    print(f"\nğŸ“Š å¤‰æ›å®Œäº†: {converted_count}å€‹ã®é…ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã¾ã—ãŸ")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}/")
    
    # ä½¿ç”¨æ–¹æ³•ã‚’è¡¨ç¤º
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"python import_comments_to_db.py --root-dir {output_dir}")

def create_directory_structure(base_dir: str, user_id: str, user_name: str):
    """SpecialUserå½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ"""
    
    special_user_dir = os.path.join(base_dir, "SpecialUser")
    user_dir = os.path.join(special_user_dir, f"{user_id}_{user_name}")
    
    os.makedirs(user_dir, exist_ok=True)
    
    return special_user_dir, user_dir

def convert_to_specialuser_format(json_dir: str, output_base: str = "SpecialUser_converted"):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’SpecialUserå½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«é…ç½®"""
    
    user_id = "21639740"
    user_name = "ãƒãƒ³ã‚«ã‚¹ã‚¢ãƒ‹ãƒ¡è±šå¤ªéƒ"
    
    special_user_dir, user_dir = create_directory_structure(output_base, user_id, user_name)
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    json_files = [f for f in os.listdir(json_dir) if f.endswith('_comments.json')]
    
    for json_file in json_files:
        json_path = os.path.join(json_dir, json_file)
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        lv_value = data['broadcast_info']['lv_value']
        lv_dir = os.path.join(user_dir, lv_value)
        os.makedirs(lv_dir, exist_ok=True)
        
        # comments.jsonã¨ã—ã¦ä¿å­˜
        output_path = os.path.join(lv_dir, 'comments.json')
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ é…ç½®å®Œäº†: {lv_value}")
    
    print(f"\nâœ… SpecialUserå½¢å¼ã§ã®é…ç½®å®Œäº†")
    print(f"å‡ºåŠ›å…ˆ: {output_base}/")
    print(f"\nğŸ’¡ ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–¹æ³•:")
    print(f"python import_comments_to_db.py --root-dir {output_base}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='HTMLã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã«å¤‰æ›')
    parser.add_argument('html_file', help='å…¥åŠ›HTMLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--output-dir', default='converted_data', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--create-specialuser', action='store_true', 
                        help='SpecialUserå½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚‚ä½œæˆ')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.html_file):
        print(f"âŒ HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.html_file}")
        exit(1)
    
    print(f"ğŸ”„ HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­: {args.html_file}")
    
    # JSONå½¢å¼ã«å¤‰æ›
    parse_html_to_json(args.html_file, args.output_dir)
    
    # SpecialUserå½¢å¼ã‚‚ä½œæˆã™ã‚‹å ´åˆ
    if args.create_specialuser:
        print(f"\nğŸ”„ SpecialUserå½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆä¸­...")
        convert_to_specialuser_format(args.output_dir)