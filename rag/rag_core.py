# rag/rag_core.pyï¼ˆå®Œæˆå½¢ï¼‰
import sqlite3
import numpy as np
import os
from typing import List, Dict, Optional
from .statistical import StatisticalAnalyzer

class RAGSystem:
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, main_db_path="data/ncv_monitor.db", vector_db_path="data/vectors.db"):
        self.main_db_path = main_db_path
        self.vector_db_path = vector_db_path
        
        # çµ±è¨ˆåˆ†ææ©Ÿèƒ½
        self.statistical_analyzer = StatisticalAnalyzer(main_db_path)
        
        # DBã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not os.path.exists(self.vector_db_path):
            print(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«DBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.vector_db_path}")
        
        if not os.path.exists(self.main_db_path):
            print(f"âš ï¸ ãƒ¡ã‚¤ãƒ³DBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.main_db_path}")
    
    def query(self, question: str, user_id: Optional[str] = None, 
              broadcast_id: Optional[int] = None, top_k: int = 5) -> Dict:
        """ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªå‡¦ç†"""
        
        print(f"ğŸ” RAG Query: '{question}'")
        if user_id:
            print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {user_id}")
        if broadcast_id:
            print(f"   æ”¾é€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {broadcast_id}")
        
        # è³ªå•ã®ç¨®é¡ã‚’åˆ¤å®š
        query_type = self._classify_question(question)
        print(f"   ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—: {query_type}")
        
        if query_type == "statistical":
            return self._handle_statistical_query(question, user_id, broadcast_id)
        else:
            return self._handle_semantic_query(question, user_id, broadcast_id, top_k)
    
    def _classify_question(self, question: str) -> str:
        """è³ªå•ã®ç¨®é¡ã‚’åˆ†é¡"""
        statistical_keywords = [
            "ã‚ˆã", "é »ç¹", "å¤šã", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "é †ä½", "å›æ•°", "çµ±è¨ˆ", 
            "å‚¾å‘", "ãƒ‘ã‚¿ãƒ¼ãƒ³", "åˆ†æ", "ã©ã®ãã‚‰ã„", "ä½•å›", "ã©ã®é…ä¿¡è€…",
            "ä½•äºº", "èª°ãŒ", "ã©ã“ã®", "ã„ã¤", "æ™‚é–“å¸¯", "æœˆ", "æ—¥"
        ]
        
        question_lower = question.lower()
        for keyword in statistical_keywords:
            if keyword in question_lower:
                return "statistical"
        
        return "semantic"
    
    def _handle_statistical_query(self, question: str, user_id: Optional[str], 
                                 broadcast_id: Optional[int]) -> Dict:
        """çµ±è¨ˆçš„è³ªå•ã¸ã®å¯¾å¿œ"""
        print("ğŸ“Š çµ±è¨ˆåˆ†æãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†ä¸­...")
        return self.statistical_analyzer.analyze(question, user_id, broadcast_id)
    
    def _handle_semantic_query(self, question: str, user_id: Optional[str], 
                              broadcast_id: Optional[int], top_k: int) -> Dict:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰"""
        
        print("ğŸ” ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†ä¸­...")
        
        # 1. è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        question_vector = self._get_embedding(question)
        if question_vector is None:
            return {
                'answer': "âŒ è³ªå•ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚OpenAI APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                'sources': [],
                'query_type': 'semantic',
                'error': 'embedding_failed'
            }
        
        # 2. é¡ä¼¼æ¤œç´¢
        similar_comments = self._search_similar_comments(question_vector, top_k, user_id, broadcast_id)
        similar_analyses = self._search_similar_analyses(question_vector, top_k, user_id, broadcast_id)
        
        print(f"   æ¤œç´¢çµæœ: ã‚³ãƒ¡ãƒ³ãƒˆ{len(similar_comments)}ä»¶, AIåˆ†æ{len(similar_analyses)}ä»¶")
        
        # 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context = self._build_context(similar_comments, similar_analyses)
        
        if not context.strip():
            return {
                'answer': "ğŸ¤· æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã§é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
                'sources': [],
                'query_type': 'semantic',
                'filters_applied': {'user_id': user_id, 'broadcast_id': broadcast_id}
            }
        
        # 4. å›ç­”ç”Ÿæˆ
        answer = self._generate_answer(question, context, user_id, broadcast_id)
        sources = self._format_sources(similar_comments, similar_analyses)
        
        return {
            'answer': answer,
            'sources': sources,
            'query_type': 'semantic',
            'total_sources': len(sources),
            'filters_applied': {'user_id': user_id, 'broadcast_id': broadcast_id}
        }
    
    def _get_embedding(self, text: str) -> Optional[np.ndarray]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        try:
            import openai
            
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆç’°å¢ƒå¤‰æ•° OPENAI_API_KEYï¼‰")
                return None
            
            client = openai.OpenAI(api_key=api_key)
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            
            vector = np.array(response.data[0].embedding, dtype=np.float32)
            print(f"   âœ… ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†: {len(vector)}æ¬¡å…ƒ")
            return vector
            
        except ImportError:
            print("âŒ openaiãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: pip install openai")
            return None
        except Exception as e:
            print(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _search_similar_comments(self, query_vector: np.ndarray, top_k: int,
                                user_id: Optional[str] = None, 
                                broadcast_id: Optional[int] = None) -> List[Dict]:
        """é¡ä¼¼ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢"""
        try:
            if not os.path.exists(self.vector_db_path):
                print("âŒ ãƒ™ã‚¯ãƒˆãƒ«DBãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return []
            
            with sqlite3.connect(self.vector_db_path) as conn:
                cursor = conn.cursor()
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’å‹•çš„ã«æ§‹ç¯‰
                query = """
                    SELECT cv.comment_id, cv.user_id, cv.comment_text, cv.vector_data, cv.broadcast_id
                    FROM comment_vectors cv
                    WHERE 1=1
                """
                params = []
                
                if user_id:
                    query += " AND cv.user_id = ?"
                    params.append(user_id)
                
                if broadcast_id:
                    query += " AND cv.broadcast_id = ?"
                    params.append(broadcast_id)
                
                cursor.execute(query, params)
                rows = cursor.fetchall()
                
                if not rows:
                    print("   ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return []
                
                print(f"   æ¤œç´¢å¯¾è±¡ãƒ™ã‚¯ãƒˆãƒ«: {len(rows)}ä»¶")
                
                results = []
                for row in rows:
                    comment_id, uid, comment_text, vector_blob, bid = row
                    
                    try:
                        stored_vector = np.frombuffer(vector_blob, dtype=np.float32)
                        similarity = self._cosine_similarity(query_vector, stored_vector)
                        
                        results.append({
                            'comment_id': comment_id,
                            'user_id': uid,
                            'comment_text': comment_text,
                            'broadcast_id': bid,
                            'similarity': similarity,
                            'type': 'comment'
                        })
                    except Exception as e:
                        print(f"   ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒã‚¨ãƒ©ãƒ¼ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆID: {comment_id}ï¼‰: {str(e)}")
                        continue
            
            # é¡ä¼¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            results.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = results[:top_k]
            
            # è¿½åŠ æƒ…å ±ã‚’å–å¾—
            enriched_results = self._enrich_comment_results(top_results)
            
            print(f"   é¡ä¼¼ã‚³ãƒ¡ãƒ³ãƒˆ: {len(enriched_results)}ä»¶ï¼ˆæœ€é«˜é¡ä¼¼åº¦: {top_results[0]['similarity']:.3f if top_results else 0}ï¼‰")
            return enriched_results
            
        except Exception as e:
            print(f"âŒ ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _search_similar_analyses(self, query_vector: np.ndarray, top_k: int,
                                user_id: Optional[str] = None,
                                broadcast_id: Optional[int] = None) -> List[Dict]:
        """é¡ä¼¼AIåˆ†ææ¤œç´¢"""
        try:
            if not os.path.exists(self.vector_db_path):
                return []
            
            with sqlite3.connect(self.vector_db_path) as conn:
                cursor = conn.cursor()
                
                query = """
                    SELECT av.analysis_id, av.user_id, av.analysis_text, av.vector_data, av.broadcast_id
                    FROM analysis_vectors av
                    WHERE 1=1
                """
                params = []
                
                if user_id:
                    query += " AND av.user_id = ?"
                    params.append(user_id)
                
                if broadcast_id:
                    query += " AND av.broadcast_id = ?"
                    params.append(broadcast_id)
                
                cursor.execute(query, params)
                rows = cursor.fetchall()
                
                if not rows:
                    print("   ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸAIåˆ†æãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return []
                
                print(f"   æ¤œç´¢å¯¾è±¡AIåˆ†æ: {len(rows)}ä»¶")
                
                results = []
                for row in rows:
                    analysis_id, uid, analysis_text, vector_blob, bid = row
                    
                    try:
                        stored_vector = np.frombuffer(vector_blob, dtype=np.float32)
                        similarity = self._cosine_similarity(query_vector, stored_vector)
                        
                        results.append({
                            'analysis_id': analysis_id,
                            'user_id': uid,
                            'analysis_text': analysis_text,
                            'broadcast_id': bid,
                            'similarity': similarity,
                            'type': 'analysis'
                        })
                    except Exception as e:
                        print(f"   ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒã‚¨ãƒ©ãƒ¼ï¼ˆAIåˆ†æID: {analysis_id}ï¼‰: {str(e)}")
                        continue
            
            results.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = results[:top_k]
            
            enriched_results = self._enrich_analysis_results(top_results)
            
            print(f"   é¡ä¼¼AIåˆ†æ: {len(enriched_results)}ä»¶")
            return enriched_results
            
        except Exception as e:
            print(f"âŒ AIåˆ†ææ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _enrich_comment_results(self, results: List[Dict]) -> List[Dict]:
        """ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢çµæœã«è¿½åŠ æƒ…å ±ã‚’ä»˜ä¸"""
        if not results:
            return results
        
        try:
            comment_ids = [r['comment_id'] for r in results]
            placeholders = ','.join(['?' for _ in comment_ids])
            
            with sqlite3.connect(self.main_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(f"""
                    SELECT c.id, c.user_name, c.timestamp, c.elapsed_time,
                           b.lv_value, b.live_title, b.start_time,
                           su.display_name
                    FROM comments c
                    JOIN broadcasts b ON c.broadcast_id = b.id
                    LEFT JOIN special_users su ON c.user_id = su.user_id
                    WHERE c.id IN ({placeholders})
                """, comment_ids)
                
                enrichment_data = {}
                for row in cursor.fetchall():
                    comment_id, user_name, timestamp, elapsed_time, lv_value, live_title, start_time, display_name = row
                    enrichment_data[comment_id] = {
                        'user_name': user_name,
                        'display_name': display_name or user_name,
                        'timestamp': timestamp,
                        'elapsed_time': elapsed_time,
                        'lv_value': lv_value,
                        'live_title': live_title,
                        'start_time': start_time
                    }
            
            # çµæœã«è¿½åŠ æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
            for result in results:
                comment_id = result['comment_id']
                if comment_id in enrichment_data:
                    result.update(enrichment_data[comment_id])
            
            return results
            
        except Exception as e:
            print(f"âŒ ã‚³ãƒ¡ãƒ³ãƒˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return results
    
    def _enrich_analysis_results(self, results: List[Dict]) -> List[Dict]:
        """AIåˆ†ææ¤œç´¢çµæœã«è¿½åŠ æƒ…å ±ã‚’ä»˜ä¸"""
        if not results:
            return results
        
        try:
            analysis_ids = [r['analysis_id'] for r in results]
            placeholders = ','.join(['?' for _ in analysis_ids])
            
            with sqlite3.connect(self.main_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(f"""
                    SELECT a.id, a.model_used, a.comment_count, a.analysis_date,
                           b.lv_value, b.live_title, b.start_time,
                           su.display_name
                    FROM ai_analyses a
                    JOIN broadcasts b ON a.broadcast_id = b.id
                    LEFT JOIN special_users su ON a.user_id = su.user_id
                    WHERE a.id IN ({placeholders})
                """, analysis_ids)
                
                enrichment_data = {}
                for row in cursor.fetchall():
                    analysis_id, model_used, comment_count, analysis_date, lv_value, live_title, start_time, display_name = row
                    enrichment_data[analysis_id] = {
                        'model_used': model_used,
                        'comment_count': comment_count,
                        'analysis_date': analysis_date,
                        'lv_value': lv_value,
                        'live_title': live_title,
                        'start_time': start_time,
                        'display_name': display_name
                    }
            
            for result in results:
                analysis_id = result['analysis_id']
                if analysis_id in enrichment_data:
                    result.update(enrichment_data[analysis_id])
            
            return results
            
        except Exception as e:
            print(f"âŒ AIåˆ†ææƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return results
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm_a = np.linalg.norm(vec1)
            norm_b = np.linalg.norm(vec2)
            
            if norm_a == 0 or norm_b == 0:
                return 0.0
                
            return float(dot_product / (norm_a * norm_b))
            
        except Exception as e:
            print(f"âŒ é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return 0.0
    
    def _build_context(self, comments: List[Dict], analyses: List[Dict]) -> str:
        """æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        context_parts = []
        
        # ã‚³ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’è¿½åŠ 
        if comments:
            context_parts.append("ã€é–¢é€£ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã€‘")
            for i, comment in enumerate(comments, 1):
                user_name = comment.get('display_name', comment.get('user_name', 'ä¸æ˜'))
                live_title = comment.get('live_title', 'ä¸æ˜ãªé…ä¿¡')
                elapsed_time = comment.get('elapsed_time', 'ä¸æ˜')
                similarity = comment.get('similarity', 0)
                
                context_parts.append(
                    f"{i}. {user_name}: ã€Œ{comment['comment_text']}ã€"
                    f"\n   é…ä¿¡: {live_title} ({elapsed_time}) [é¡ä¼¼åº¦: {similarity:.3f}]"
                )
        
        # AIåˆ†ææƒ…å ±ã‚’è¿½åŠ 
        if analyses:
            context_parts.append("\nã€é–¢é€£ã™ã‚‹AIåˆ†æã€‘")
            for i, analysis in enumerate(analyses, 1):
                user_name = analysis.get('display_name', f"ãƒ¦ãƒ¼ã‚¶ãƒ¼{analysis['user_id']}")
                live_title = analysis.get('live_title', 'ä¸æ˜ãªé…ä¿¡')
                model_used = analysis.get('model_used', 'ä¸æ˜')
                similarity = analysis.get('similarity', 0)
                
                # åˆ†æçµæœã¯é•·ã„ã®ã§è¦ç´„
                analysis_preview = analysis['analysis_text'][:200] + "..."
                
                context_parts.append(
                    f"{i}. {user_name}ã®åˆ†æ (by {model_used}): {analysis_preview}"
                    f"\n   é…ä¿¡: {live_title} [é¡ä¼¼åº¦: {similarity:.3f}]"
                )
        
        return "\n\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str, 
                        user_id: Optional[str], broadcast_id: Optional[int]) -> str:
        """LLMã§å›ç­”ç”Ÿæˆ"""
        try:
            import openai
            
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                return "âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆç’°å¢ƒå¤‰æ•° OPENAI_API_KEYï¼‰"
            
            client = openai.OpenAI(api_key=api_key)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æƒ…å ±ã‚’å«ã‚€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            filter_info = []
            if user_id:
                filter_info.append(f"ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_id}")
            if broadcast_id:
                filter_info.append(f"ç‰¹å®šæ”¾é€: {broadcast_id}")
            
            filter_text = f"ï¼ˆæ¤œç´¢æ¡ä»¶: {', '.join(filter_info)}ï¼‰" if filter_info else ""
            
            system_prompt = f"""ã‚ãªãŸã¯ãƒ‹ã‚³ãƒ‹ã‚³ç”Ÿæ”¾é€ã®åˆ†æå°‚é–€å®¶ã§ã™ã€‚
é…ä¿¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚„AIåˆ†æçµæœã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

{filter_text}

å›ç­”ã®éš›ã¯ä»¥ä¸‹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ï¼š
- æä¾›ã•ã‚ŒãŸæƒ…å ±ã‚’å…·ä½“çš„ã«å¼•ç”¨ã™ã‚‹
- æ¨æ¸¬ã¨äº‹å®Ÿã‚’æ˜ç¢ºã«åŒºåˆ¥ã™ã‚‹
- æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ãã®æ—¨ã‚’è¿°ã¹ã‚‹
- æ¤œç´¢æ¡ä»¶ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ç¯„å›²å†…ã§ã®å›ç­”ã§ã‚ã‚‹ã“ã¨ã‚’æ˜è¨˜ã™ã‚‹
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã™ã‚‹"""

            user_prompt = f"""
è³ªå•: {question}

å‚è€ƒæƒ…å ±:
{context}

ä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€è³ªå•ã«å¯¾ã™ã‚‹è©³ç´°ã§æœ‰ç”¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
            
            print("ğŸ¤– AIå›ç­”ç”Ÿæˆä¸­...")
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content.strip()
            print("âœ… AIå›ç­”ç”Ÿæˆå®Œäº†")
            
            return answer
            
        except ImportError:
            return "âŒ openaiãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: pip install openai"
        except Exception as e:
            print(f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def _format_sources(self, comments: List[Dict], analyses: List[Dict]) -> List[Dict]:
        """ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ•´å½¢"""
        sources = []
        
        for comment in comments:
            sources.append({
                'type': 'comment',
                'content': comment['comment_text'],
                'user_name': comment.get('display_name', comment.get('user_name', 'ä¸æ˜')),
                'user_id': comment['user_id'],
                'live_title': comment.get('live_title', 'ä¸æ˜'),
                'lv_value': comment.get('lv_value', 'ä¸æ˜'),
                'elapsed_time': comment.get('elapsed_time', 'ä¸æ˜'),
                'similarity': comment.get('similarity', 0)
            })
        
        for analysis in analyses:
            sources.append({
                'type': 'analysis',
                'content': analysis['analysis_text'][:200] + "...",
                'user_name': analysis.get('display_name', f"ãƒ¦ãƒ¼ã‚¶ãƒ¼{analysis['user_id']}"),
                'user_id': analysis['user_id'],
                'live_title': analysis.get('live_title', 'ä¸æ˜'),
                'lv_value': analysis.get('lv_value', 'ä¸æ˜'),
                'model_used': analysis.get('model_used', 'ä¸æ˜'),
                'similarity': analysis.get('similarity', 0)
            })
        
        return sources
    
    def get_status(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³å–å¾—"""
        status = {
            'vector_db_exists': os.path.exists(self.vector_db_path),
            'main_db_exists': os.path.exists(self.main_db_path),
            'total_comment_vectors': 0,
            'total_analysis_vectors': 0,
            'unique_broadcasts': 0,
            'unique_users': 0
        }
        
        if status['vector_db_exists']:
            try:
                with sqlite3.connect(self.vector_db_path) as conn:
                    cursor = conn.cursor()
                    
                    cursor.execute("SELECT COUNT(*) FROM comment_vectors")
                    status['total_comment_vectors'] = cursor.fetchone()[0]
                    
                    cursor.execute("SELECT COUNT(*) FROM analysis_vectors")
                    status['total_analysis_vectors'] = cursor.fetchone()[0]
                    
                    cursor.execute("SELECT COUNT(DISTINCT broadcast_id) FROM comment_vectors")
                    status['unique_broadcasts'] = cursor.fetchone()[0]
                    
                    cursor.execute("SELECT COUNT(DISTINCT user_id) FROM comment_vectors")
                    status['unique_users'] = cursor.fetchone()[0]
            except Exception as e:
                print(f"âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return status
    
    def search_by_user(self, user_id: str, limit: int = 10) -> Dict:
        """ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æƒ…å ±ã‚’æ¤œç´¢"""
        return self.query(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼{user_id}ã®æƒ…å ±", user_id=user_id, top_k=limit)
    
    def search_by_broadcast(self, broadcast_id: int, limit: int = 10) -> Dict:
        """ç‰¹å®šæ”¾é€ã®æƒ…å ±ã‚’æ¤œç´¢"""
        return self.query("ã“ã®æ”¾é€ã®æƒ…å ±", broadcast_id=broadcast_id, top_k=limit)