# rag/rag_core.pyÔºàÂÆåÊàêÂΩ¢Ôºâ
import sqlite3
import numpy as np
import os
from typing import List, Dict, Optional
from .statistical import StatisticalAnalyzer

class RAGSystem:
    """RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆ„É°„Ç§„É≥„ÇØ„É©„Çπ"""
    
    def __init__(self, main_db_path="data/ncv_monitor.db", vector_db_path="data/vectors.db"):
        self.main_db_path = main_db_path
        self.vector_db_path = vector_db_path
        
        # Áµ±Ë®àÂàÜÊûêÊ©üËÉΩ
        self.statistical_analyzer = StatisticalAnalyzer(main_db_path)
        
        # DB„ÅÆÂ≠òÂú®„ÉÅ„Çß„ÉÉ„ÇØ
        if not os.path.exists(self.vector_db_path):
            print(f"‚ö†Ô∏è „Éô„ÇØ„Éà„É´DB„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {self.vector_db_path}")
        
        if not os.path.exists(self.main_db_path):
            print(f"‚ö†Ô∏è „É°„Ç§„É≥DB„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {self.main_db_path}")
    
    def query(self, question: str, user_id: Optional[str] = None, 
              broadcast_id: Optional[int] = None, top_k: int = 5) -> Dict:
        """„É°„Ç§„É≥„ÇØ„Ç®„É™Âá¶ÁêÜ"""
        
        print(f"üîç RAG Query: '{question}'")
        if user_id:
            print(f"   „É¶„Éº„Ç∂„Éº„Éï„Ç£„É´„Çø„Éº: {user_id}")
        if broadcast_id:
            print(f"   ÊîæÈÄÅ„Éï„Ç£„É´„Çø„Éº: {broadcast_id}")
        
        # Ë≥™Âïè„ÅÆÁ®ÆÈ°û„ÇíÂà§ÂÆö
        query_type = self._classify_question(question)
        print(f"   „ÇØ„Ç®„É™„Çø„Ç§„Éó: {query_type}")
        
        if query_type == "statistical":
            return self._handle_statistical_query(question, user_id, broadcast_id)
        else:
            return self._handle_semantic_query(question, user_id, broadcast_id, top_k)
    
    def _classify_question(self, question: str) -> str:
        """Ë≥™Âïè„ÅÆÁ®ÆÈ°û„ÇíÂàÜÈ°û"""
        statistical_keywords = [
            "„Çà„Åè", "È†ªÁπÅ", "Â§ö„Åè", "„É©„É≥„Ç≠„É≥„Ç∞", "È†Ü‰Ωç", "ÂõûÊï∞", "Áµ±Ë®à", 
            "ÂÇæÂêë", "„Éë„Çø„Éº„É≥", "ÂàÜÊûê", "„Å©„ÅÆ„Åè„Çâ„ÅÑ", "‰ΩïÂõû", "„Å©„ÅÆÈÖç‰ø°ËÄÖ",
            "‰Ωï‰∫∫", "Ë™∞„Åå", "„Å©„Åì„ÅÆ", "„ÅÑ„Å§", "ÊôÇÈñìÂ∏Ø", "Êúà", "Êó•"
        ]
        
        question_lower = question.lower()
        for keyword in statistical_keywords:
            if keyword in question_lower:
                return "statistical"
        
        return "semantic"
    
    def _handle_statistical_query(self, question: str, user_id: Optional[str], 
                                 broadcast_id: Optional[int]) -> Dict:
        """Áµ±Ë®àÁöÑË≥™Âïè„Å∏„ÅÆÂØæÂøú"""
        print("üìä Áµ±Ë®àÂàÜÊûê„É¢„Éº„Éâ„ÅßÂá¶ÁêÜ‰∏≠...")
        return self.statistical_analyzer.analyze(question, user_id, broadcast_id)
    
    def _handle_semantic_query(self, question: str, user_id: Optional[str], 
                              broadcast_id: Optional[int], top_k: int) -> Dict:
        """„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØÊ§úÁ¥¢ÔºàÈ°û‰ººÂ∫¶„Éô„Éº„ÇπÔºâ"""
        
        print("üîç „Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØÊ§úÁ¥¢„É¢„Éº„Éâ„ÅßÂá¶ÁêÜ‰∏≠...")
        
        # 1. Ë≥™Âïè„Çí„Éô„ÇØ„Éà„É´Âåñ
        question_vector = self._get_embedding(question)
        if question_vector is None:
            return {
                'answer': "‚ùå Ë≥™Âïè„ÅÆ„Éô„ÇØ„Éà„É´Âåñ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇOpenAI API„Ç≠„Éº„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
                'sources': [],
                'query_type': 'semantic',
                'error': 'embedding_failed'
            }
        
        # 2. È°û‰ººÊ§úÁ¥¢
        similar_comments = self._search_similar_comments(question_vector, top_k, user_id, broadcast_id)
        similar_analyses = self._search_similar_analyses(question_vector, top_k, user_id, broadcast_id)
        
        print(f"   Ê§úÁ¥¢ÁµêÊûú: „Ç≥„É°„É≥„Éà{len(similar_comments)}‰ª∂, AIÂàÜÊûê{len(similar_analyses)}‰ª∂")
        
        # 3. „Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊßãÁØâ
        context = self._build_context(similar_comments, similar_analyses)
        
        if not context.strip():
            return {
                'answer': "ü§∑ ÊåáÂÆö„Åï„Çå„ÅüÊù°‰ª∂„ÅßÈñ¢ÈÄ£„Åô„ÇãÊÉÖÂ†±„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ",
                'sources': [],
                'query_type': 'semantic',
                'filters_applied': {'user_id': user_id, 'broadcast_id': broadcast_id}
            }
        
        # 4. ÂõûÁ≠îÁîüÊàê
        answer = self._generate_answer(question, context, user_id, broadcast_id)
        sources = self._format_sources(similar_comments, similar_analyses)
        
        return {
            'answer': answer,
            'sources': sources,
            'query_type': 'semantic',
            'total_sources': len(sources),
            'filters_applied': {'user_id': user_id, 'broadcast_id': broadcast_id}
        }
    
    def _get_embedding(self, text: str) -> Optional[np.ndarray]:
        """„ÉÜ„Ç≠„Çπ„Éà„Çí„Éô„ÇØ„Éà„É´Âåñ"""
        try:
            import openai
            
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                print("‚ùå OpenAI API„Ç≠„Éº„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„ÇìÔºàÁí∞Â¢ÉÂ§âÊï∞ OPENAI_API_KEYÔºâ")
                return None
            
            client = openai.OpenAI(api_key=api_key)
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            
            vector = np.array(response.data[0].embedding, dtype=np.float32)
            print(f"   ‚úÖ „Éô„ÇØ„Éà„É´ÂåñÂÆå‰∫Ü: {len(vector)}Ê¨°ÂÖÉ")
            return vector
            
        except ImportError:
            print("‚ùå openai„É©„Ç§„Éñ„É©„É™„Åå„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì: pip install openai")
            return None
        except Exception as e:
            print(f"‚ùå „Éô„ÇØ„Éà„É´Âåñ„Ç®„É©„Éº: {str(e)}")
            return None
    
    def _search_similar_comments(self, query_vector: np.ndarray, top_k: int,
                                user_id: Optional[str] = None, 
                                broadcast_id: Optional[int] = None) -> List[Dict]:
        """È°û‰ºº„Ç≥„É°„É≥„ÉàÊ§úÁ¥¢"""
        try:
            if not os.path.exists(self.vector_db_path):
                print("‚ùå „Éô„ÇØ„Éà„É´DB„ÅåÂ≠òÂú®„Åó„Åæ„Åõ„Çì")
                return []
            
            with sqlite3.connect(self.vector_db_path) as conn:
                cursor = conn.cursor()
                
                # „Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂„ÇíÂãïÁöÑ„Å´ÊßãÁØâ
                query = """
                    SELECT cv.comment_id, cv.user_id, cv.comment_text, cv.vector_data, cv.broadcast_id
                    FROM comment_vectors cv
                    WHERE 1=1
                """
                params = []
                
                if user_id:
                    query += " AND cv.user_id = ?"
                    params.append(user_id)
                
                if broadcast_id:
                    query += " AND cv.broadcast_id = ?"
                    params.append(broadcast_id)
                
                cursor.execute(query, params)
                rows = cursor.fetchall()
                
                if not rows:
                    print("   „Éô„ÇØ„Éà„É´Âåñ„Åï„Çå„Åü„Ç≥„É°„É≥„Éà„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
                    return []
                
                print(f"   Ê§úÁ¥¢ÂØæË±°„Éô„ÇØ„Éà„É´: {len(rows)}‰ª∂")
                
                results = []
                for row in rows:
                    comment_id, uid, comment_text, vector_blob, bid = row
                    
                    try:
                        stored_vector = np.frombuffer(vector_blob, dtype=np.float32)
                        similarity = self._cosine_similarity(query_vector, stored_vector)
                        
                        results.append({
                            'comment_id': comment_id,
                            'user_id': uid,
                            'comment_text': comment_text,
                            'broadcast_id': bid,
                            'similarity': similarity,
                            'type': 'comment'
                        })
                    except Exception as e:
                        print(f"   „Éô„ÇØ„Éà„É´Âæ©ÂÖÉ„Ç®„É©„ÉºÔºà„Ç≥„É°„É≥„ÉàID: {comment_id}Ôºâ: {str(e)}")
                        continue
            
            # È°û‰ººÂ∫¶È†Ü„Å´„ÇΩ„Éº„Éà
            results.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = results[:top_k]
            
            # ËøΩÂä†ÊÉÖÂ†±„ÇíÂèñÂæó
            enriched_results = self._enrich_comment_results(top_results)
            
            print(f"   È°û‰ºº„Ç≥„É°„É≥„Éà: {len(enriched_results)}‰ª∂ÔºàÊúÄÈ´òÈ°û‰ººÂ∫¶: {top_results[0]['similarity']:.3f if top_results else 0}Ôºâ")
            return enriched_results
            
        except Exception as e:
            print(f"‚ùå „Ç≥„É°„É≥„ÉàÊ§úÁ¥¢„Ç®„É©„Éº: {str(e)}")
            return []
    
    def _search_similar_analyses(self, query_vector: np.ndarray, top_k: int,
                                user_id: Optional[str] = None,
                                broadcast_id: Optional[int] = None) -> List[Dict]:
        """È°û‰ººAIÂàÜÊûêÊ§úÁ¥¢"""
        try:
            if not os.path.exists(self.vector_db_path):
                return []
            
            with sqlite3.connect(self.vector_db_path) as conn:
                cursor = conn.cursor()
                
                query = """
                    SELECT av.analysis_id, av.user_id, av.analysis_text, av.vector_data, av.broadcast_id
                    FROM analysis_vectors av
                    WHERE 1=1
                """
                params = []
                
                if user_id:
                    query += " AND av.user_id = ?"
                    params.append(user_id)
                
                if broadcast_id:
                    query += " AND av.broadcast_id = ?"
                    params.append(broadcast_id)
                
                cursor.execute(query, params)
                rows = cursor.fetchall()
                
                if not rows:
                    print("   „Éô„ÇØ„Éà„É´Âåñ„Åï„Çå„ÅüAIÂàÜÊûê„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
                    return []
                
                print(f"   Ê§úÁ¥¢ÂØæË±°AIÂàÜÊûê: {len(rows)}‰ª∂")
                
                results = []
                for row in rows:
                    analysis_id, uid, analysis_text, vector_blob, bid = row
                    
                    try:
                        stored_vector = np.frombuffer(vector_blob, dtype=np.float32)
                        similarity = self._cosine_similarity(query_vector, stored_vector)
                        
                        results.append({
                            'analysis_id': analysis_id,
                            'user_id': uid,
                            'analysis_text': analysis_text,
                            'broadcast_id': bid,
                            'similarity': similarity,
                            'type': 'analysis'
                        })
                    except Exception as e:
                        print(f"   „Éô„ÇØ„Éà„É´Âæ©ÂÖÉ„Ç®„É©„ÉºÔºàAIÂàÜÊûêID: {analysis_id}Ôºâ: {str(e)}")
                        continue
            
            results.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = results[:top_k]
            
            enriched_results = self._enrich_analysis_results(top_results)
            
            print(f"   È°û‰ººAIÂàÜÊûê: {len(enriched_results)}‰ª∂")
            return enriched_results
            
        except Exception as e:
            print(f"‚ùå AIÂàÜÊûêÊ§úÁ¥¢„Ç®„É©„Éº: {str(e)}")
            return []
    
    def _enrich_comment_results(self, results: List[Dict]) -> List[Dict]:
        """„Ç≥„É°„É≥„ÉàÊ§úÁ¥¢ÁµêÊûú„Å´ËøΩÂä†ÊÉÖÂ†±„Çí‰ªò‰∏é"""
        if not results:
            return results
        
        try:
            comment_ids = [r['comment_id'] for r in results]
            placeholders = ','.join(['?' for _ in comment_ids])
            
            with sqlite3.connect(self.main_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(f"""
                    SELECT c.id, c.user_name, c.timestamp, c.elapsed_time,
                           b.lv_value, b.live_title, b.start_time,
                           su.display_name
                    FROM comments c
                    JOIN broadcasts b ON c.broadcast_id = b.id
                    LEFT JOIN special_users su ON c.user_id = su.user_id
                    WHERE c.id IN ({placeholders})
                """, comment_ids)
                
                enrichment_data = {}
                for row in cursor.fetchall():
                    comment_id, user_name, timestamp, elapsed_time, lv_value, live_title, start_time, display_name = row
                    enrichment_data[comment_id] = {
                        'user_name': user_name,
                        'display_name': display_name or user_name,
                        'timestamp': timestamp,
                        'elapsed_time': elapsed_time,
                        'lv_value': lv_value,
                        'live_title': live_title,
                        'start_time': start_time
                    }
            
            # ÁµêÊûú„Å´ËøΩÂä†ÊÉÖÂ†±„Çí„Éû„Éº„Ç∏
            for result in results:
                comment_id = result['comment_id']
                if comment_id in enrichment_data:
                    result.update(enrichment_data[comment_id])
            
            return results
            
        except Exception as e:
            print(f"‚ùå „Ç≥„É°„É≥„ÉàÊÉÖÂ†±ÂèñÂæó„Ç®„É©„Éº: {str(e)}")
            return results
    
    def _enrich_analysis_results(self, results: List[Dict]) -> List[Dict]:
        """AIÂàÜÊûêÊ§úÁ¥¢ÁµêÊûú„Å´ËøΩÂä†ÊÉÖÂ†±„Çí‰ªò‰∏é"""
        if not results:
            return results
        
        try:
            analysis_ids = [r['analysis_id'] for r in results]
            placeholders = ','.join(['?' for _ in analysis_ids])
            
            with sqlite3.connect(self.main_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(f"""
                    SELECT a.id, a.model_used, a.comment_count, a.analysis_date,
                           b.lv_value, b.live_title, b.start_time,
                           su.display_name
                    FROM ai_analyses a
                    JOIN broadcasts b ON a.broadcast_id = b.id
                    LEFT JOIN special_users su ON a.user_id = su.user_id
                    WHERE a.id IN ({placeholders})
                """, analysis_ids)
                
                enrichment_data = {}
                for row in cursor.fetchall():
                    analysis_id, model_used, comment_count, analysis_date, lv_value, live_title, start_time, display_name = row
                    enrichment_data[analysis_id] = {
                        'model_used': model_used,
                        'comment_count': comment_count,
                        'analysis_date': analysis_date,
                        'lv_value': lv_value,
                        'live_title': live_title,
                        'start_time': start_time,
                        'display_name': display_name
                    }
            
            for result in results:
                analysis_id = result['analysis_id']
                if analysis_id in enrichment_data:
                    result.update(enrichment_data[analysis_id])
            
            return results
            
        except Exception as e:
            print(f"‚ùå AIÂàÜÊûêÊÉÖÂ†±ÂèñÂæó„Ç®„É©„Éº: {str(e)}")
            return results
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """„Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶Ë®àÁÆó"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm_a = np.linalg.norm(vec1)
            norm_b = np.linalg.norm(vec2)
            
            if norm_a == 0 or norm_b == 0:
                return 0.0
                
            return float(dot_product / (norm_a * norm_b))
            
        except Exception as e:
            print(f"‚ùå È°û‰ººÂ∫¶Ë®àÁÆó„Ç®„É©„Éº: {str(e)}")
            return 0.0
    
    def _build_context(self, comments: List[Dict], analyses: List[Dict]) -> str:
        """Ê§úÁ¥¢ÁµêÊûú„Åã„Çâ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÇíÊßãÁØâ"""
        context_parts = []
        
        # „Ç≥„É°„É≥„ÉàÊÉÖÂ†±„ÇíËøΩÂä†
        if comments:
            context_parts.append("„ÄêÈñ¢ÈÄ£„Åô„Çã„Ç≥„É°„É≥„Éà„Äë")
            for i, comment in enumerate(comments, 1):
                user_name = comment.get('display_name', comment.get('user_name', '‰∏çÊòé'))
                live_title = comment.get('live_title', '‰∏çÊòé„Å™ÈÖç‰ø°')
                elapsed_time = comment.get('elapsed_time', '‰∏çÊòé')
                similarity = comment.get('similarity', 0)
                
                context_parts.append(
                    f"{i}. {user_name}: „Äå{comment['comment_text']}„Äç"
                    f"\n   ÈÖç‰ø°: {live_title} ({elapsed_time}) [È°û‰ººÂ∫¶: {similarity:.3f}]"
                )
        
        # AIÂàÜÊûêÊÉÖÂ†±„ÇíËøΩÂä†
        if analyses:
            context_parts.append("\n„ÄêÈñ¢ÈÄ£„Åô„ÇãAIÂàÜÊûê„Äë")
            for i, analysis in enumerate(analyses, 1):
                user_name = analysis.get('display_name', f"„É¶„Éº„Ç∂„Éº{analysis['user_id']}")
                live_title = analysis.get('live_title', '‰∏çÊòé„Å™ÈÖç‰ø°')
                model_used = analysis.get('model_used', '‰∏çÊòé')
                similarity = analysis.get('similarity', 0)
                
                # ÂàÜÊûêÁµêÊûú„ÅØÈï∑„ÅÑ„ÅÆ„ÅßË¶ÅÁ¥Ñ
                analysis_preview = analysis['analysis_text'][:200] + "..."
                
                context_parts.append(
                    f"{i}. {user_name}„ÅÆÂàÜÊûê (by {model_used}): {analysis_preview}"
                    f"\n   ÈÖç‰ø°: {live_title} [È°û‰ººÂ∫¶: {similarity:.3f}]"
                )
        
        return "\n\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str, 
                        user_id: Optional[str], broadcast_id: Optional[int]) -> str:
        """LLM„ÅßÂõûÁ≠îÁîüÊàê"""
        try:
            import openai
            
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                return "‚ùå OpenAI API„Ç≠„Éº„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„ÇìÔºàÁí∞Â¢ÉÂ§âÊï∞ OPENAI_API_KEYÔºâ"
            
            client = openai.OpenAI(api_key=api_key)
            
            # „Éï„Ç£„É´„Çø„ÉºÊÉÖÂ†±„ÇíÂê´„ÇÄ„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà
            filter_info = []
            if user_id:
                filter_info.append(f"ÁâπÂÆö„É¶„Éº„Ç∂„Éº: {user_id}")
            if broadcast_id:
                filter_info.append(f"ÁâπÂÆöÊîæÈÄÅ: {broadcast_id}")
            
            filter_text = f"ÔºàÊ§úÁ¥¢Êù°‰ª∂: {', '.join(filter_info)}Ôºâ" if filter_info else ""
            
            system_prompt = f"""„ÅÇ„Å™„Åü„ÅØ„Éã„Ç≥„Éã„Ç≥ÁîüÊîæÈÄÅ„ÅÆÂàÜÊûêÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ
ÈÖç‰ø°„ÅÆ„Ç≥„É°„É≥„Éà„ÇÑAIÂàÜÊûêÁµêÊûú„ÇíÂü∫„Å´„ÄÅ„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

{filter_text}

ÂõûÁ≠î„ÅÆÈöõ„ÅØ‰ª•‰∏ã„ÇíÂøÉ„Åå„Åë„Å¶„Åè„Å†„Åï„ÅÑÔºö
- Êèê‰æõ„Åï„Çå„ÅüÊÉÖÂ†±„ÇíÂÖ∑‰ΩìÁöÑ„Å´ÂºïÁî®„Åô„Çã
- Êé®Ê∏¨„Å®‰∫ãÂÆü„ÇíÊòéÁ¢∫„Å´Âå∫Âà•„Åô„Çã
- ÊÉÖÂ†±„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„Åù„ÅÆÊó®„ÇíËø∞„Åπ„Çã
- Ê§úÁ¥¢Êù°‰ª∂„ÅåÈÅ©Áî®„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„Åù„ÅÆÁØÑÂõ≤ÂÜÖ„Åß„ÅÆÂõûÁ≠î„Åß„ÅÇ„Çã„Åì„Å®„ÇíÊòéË®ò„Åô„Çã
- Á∞°ÊΩî„ÅßÂàÜ„Åã„Çä„ÇÑ„Åô„ÅÑÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åô„Çã"""

            user_prompt = f"""
Ë≥™Âïè: {question}

ÂèÇËÄÉÊÉÖÂ†±:
{context}

‰∏äË®ò„ÅÆÊÉÖÂ†±„ÇíÂü∫„Å´„ÄÅË≥™Âïè„Å´ÂØæ„Åô„ÇãË©≥Á¥∞„ÅßÊúâÁî®„Å™ÂõûÁ≠î„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
"""
            
            print("ü§ñ AIÂõûÁ≠îÁîüÊàê‰∏≠...")
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content.strip()
            print("‚úÖ AIÂõûÁ≠îÁîüÊàêÂÆå‰∫Ü")
            
            return answer
            
        except ImportError:
            return "‚ùå openai„É©„Ç§„Éñ„É©„É™„Åå„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì: pip install openai"
        except Exception as e:
            print(f"‚ùå ÂõûÁ≠îÁîüÊàê„Ç®„É©„Éº: {str(e)}")
            return f"ÂõûÁ≠îÁîüÊàê‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"
    
    def _format_sources(self, comments: List[Dict], analyses: List[Dict]) -> List[Dict]:
        """„ÇΩ„Éº„ÇπÊÉÖÂ†±„ÇíÊï¥ÂΩ¢"""
        sources = []
        
        for comment in comments:
            sources.append({
                'type': 'comment',
                'content': comment['comment_text'],
                'user_name': comment.get('display_name', comment.get('user_name', '‰∏çÊòé')),
                'user_id': comment['user_id'],
                'live_title': comment.get('live_title', '‰∏çÊòé'),
                'lv_value': comment.get('lv_value', '‰∏çÊòé'),
                'elapsed_time': comment.get('elapsed_time', '‰∏çÊòé'),
                'similarity': comment.get('similarity', 0)
            })
        
        for analysis in analyses:
            sources.append({
                'type': 'analysis',
                'content': analysis['analysis_text'][:200] + "...",
                'user_name': analysis.get('display_name', f"„É¶„Éº„Ç∂„Éº{analysis['user_id']}"),
                'user_id': analysis['user_id'],
                'live_title': analysis.get('live_title', '‰∏çÊòé'),
                'lv_value': analysis.get('lv_value', '‰∏çÊòé'),
                'model_used': analysis.get('model_used', '‰∏çÊòé'),
                'similarity': analysis.get('similarity', 0)
            })
        
        return sources
    
    def get_status(self) -> Dict:
        """„Ç∑„Çπ„ÉÜ„É†Áä∂Ê≥ÅÂèñÂæó"""
        status = {
            'vector_db_exists': os.path.exists(self.vector_db_path),
            'main_db_exists': os.path.exists(self.main_db_path),
            'total_comment_vectors': 0,
            'total_analysis_vectors': 0,
            'unique_broadcasts': 0,
            'unique_users': 0
        }
        
        if status['vector_db_exists']:
            try:
                with sqlite3.connect(self.vector_db_path) as conn:
                    cursor = conn.cursor()
                    
                    cursor.execute("SELECT COUNT(*) FROM comment_vectors")
                    status['total_comment_vectors'] = cursor.fetchone()[0]
                    
                    cursor.execute("SELECT COUNT(*) FROM analysis_vectors")
                    status['total_analysis_vectors'] = cursor.fetchone()[0]
                    
                    cursor.execute("SELECT COUNT(DISTINCT broadcast_id) FROM comment_vectors")
                    status['unique_broadcasts'] = cursor.fetchone()[0]
                    
                    cursor.execute("SELECT COUNT(DISTINCT user_id) FROM comment_vectors")
                    status['unique_users'] = cursor.fetchone()[0]
            except Exception as e:
                print(f"‚ùå „Çπ„ÉÜ„Éº„Çø„ÇπÂèñÂæó„Ç®„É©„Éº: {str(e)}")
        
        return status
    
    def search_by_user(self, user_id: str, limit: int = 10) -> Dict:
        """ÁâπÂÆö„É¶„Éº„Ç∂„Éº„ÅÆÊÉÖÂ†±„ÇíÊ§úÁ¥¢"""
        return self.query(f"„É¶„Éº„Ç∂„Éº{user_id}„ÅÆÊÉÖÂ†±", user_id=user_id, top_k=limit)
    
    def search_by_broadcast(self, broadcast_id: int, limit: int = 10) -> Dict:
        """ÁâπÂÆöÊîæÈÄÅ„ÅÆÊÉÖÂ†±„ÇíÊ§úÁ¥¢"""
        return self.query("„Åì„ÅÆÊîæÈÄÅ„ÅÆÊÉÖÂ†±", broadcast_id=broadcast_id, top_k=limit)