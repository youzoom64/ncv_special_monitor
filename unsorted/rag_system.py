# rag_system.py
import sqlite3
import numpy as np
import os
from typing import List, Dict, Tuple
import openai
import json

class RAGSearchSystem:
    def __init__(self, main_db_path="data/ncv_monitor.db", vector_db_path="data/vectors.db"):
        self.main_db_path = main_db_path
        self.vector_db_path = vector_db_path
        
        # DBã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not os.path.exists(self.vector_db_path):
            print(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«DBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.vector_db_path}")
            print("å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
    def classify_question_with_llm(self, question: str) -> Dict:
        """
        LLMã§è³ªå•æ„å›³ã¨æ¤œç´¢æ·±åº¦ã‚’åˆ†é¡
        ä¾‹: {"category": "comment_search", "search_depth": "high"}
        """
        config_path = os.path.join(os.path.dirname(__file__), "config", "ncv_special_config.json")
        api_key = None
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            api_key = config.get('api_settings', {}).get('openai_api_key', '')

        if not api_key:
            api_key = os.getenv('OPENAI_API_KEY')

        if not api_key:
            print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆåˆ†é¡ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            return {"category": "comment_search", "search_depth": "medium"}  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        client = openai.OpenAI(api_key=api_key, timeout=10.0)

        system_prompt = """ã‚ãªãŸã¯RAGæ¤œç´¢ã®è³ªå•åˆ†é¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
    ç›®çš„ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã‚’è§£æã—ã€ã‚³ãƒ¡ãƒ³ãƒˆDBã‚„é…ä¿¡DBã‹ã‚‰æ¤œç´¢ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹åˆ¤æ–­ã™ã‚‹ã“ã¨ã§ã™ã€‚

    åˆ†é¡ã¯æ¬¡ã®é€šã‚Š:
    1. category:
    - comment_search: é¢ç™½ã„ã‚³ãƒ¡ãƒ³ãƒˆã‚„ç››ã‚Šä¸ŠãŒã£ãŸã‚³ãƒ¡ãƒ³ãƒˆã‚’æ¢ã™è³ªå•
    - user_search: ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚„åå‰ã«ã¤ã„ã¦ã®è³ªå•ï¼ˆä¾‹: ã“ã®äººã©ã‚“ãªç™ºè¨€ã—ã¦ã‚‹ï¼Ÿï¼‰
    - broadcast_info: é…ä¿¡ã‚„æ”¾é€å†…å®¹ã«ã¤ã„ã¦ï¼ˆä¾‹: ã“ã®æ ã§ä½•ãŒã‚ã£ãŸï¼Ÿï¼‰
    - general: DBæ¤œç´¢ä¸è¦ã€LLMã®ã¿ã§ç­”ãˆã‚‰ã‚Œã‚‹

    2. search_depth:
    - low: å°‘é‡ã®çµæœã§ååˆ†
    - medium: é€šå¸¸é‡
    - high: ã§ãã‚‹ã ã‘å¤šãã®çµæœãŒå¿…è¦

    å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ã§:
    {"category": "comment_search", "search_depth": "medium"}"""

        user_prompt = f"è³ªå•: {question}"

        print(f"ğŸ§­ è³ªå•åˆ†é¡ä¸­ (miniãƒ¢ãƒ‡ãƒ«ä½¿ç”¨): {question}")
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=50,
            temperature=0
        )

        try:
            result = json.loads(response.choices[0].message.content.strip())
        except Exception:
            result = {"category": "comment_search", "search_depth": "medium"}

        print(f"ğŸ§­ LLMåˆ†é¡çµæœ: {result}")
        return result


    def search_and_answer(self, question: str, top_k: int = 5, user_id: str = None) -> str:
        """è³ªå•ã«å¯¾ã—ã¦RAGæ¤œç´¢ã—ã¦å›ç­”ç”Ÿæˆ"""
        print(f"ğŸ” RAGæ¤œç´¢é–‹å§‹: '{question}'")

        # 1. è³ªå•æ„å›³ã‚’åˆ†é¡ï¼ˆLLM or rule-basedï¼‰
        classification = self.classify_question_with_llm(question)
        category = classification["category"]
        depth = classification["search_depth"]
        top_k = {"low": 3, "medium": 5, "high": 10}.get(depth, 5)

        if category == "general":
            print("ğŸ§­ ã‚«ãƒ†ã‚´ãƒª: general â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¹ã‚­ãƒƒãƒ—")
            return self._generate_answer(question, "")

        # 2. è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        question_vector = self._get_embedding(question)
        if question_vector is None:
            return "âŒ è³ªå•ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ"

        # 3. é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œç´¢
        similar_comments = self._search_similar_comments(question_vector, top_k)
        if user_id:
            before_count = len(similar_comments)
            similar_comments = [c for c in similar_comments if str(c['user_id']) == str(user_id)]
            print(f"ğŸ§­ user_idãƒ•ã‚£ãƒ«ã‚¿é©ç”¨: {before_count} â†’ {len(similar_comments)} ä»¶")

        similar_analyses = self._search_similar_analyses(question_vector, top_k)

        print(f"ğŸ“Š æ¤œç´¢çµæœ: ã‚³ãƒ¡ãƒ³ãƒˆ{len(similar_comments)}ä»¶, AIåˆ†æ{len(similar_analyses)}ä»¶")

        # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context = self._build_context(similar_comments, similar_analyses)

        if not context.strip():
            return "ğŸ¤· é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"

        # 5. LLMã§å›ç­”ç”Ÿæˆ
        answer = self._generate_answer(question, context)
        return answer

        
    def _get_embedding(self, text: str) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        try:
            import openai
            import json
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
            config_path = os.path.join(os.path.dirname(__file__), "config", "ncv_special_config.json")

            api_key = None
            
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                api_key = config.get('api_settings', {}).get('openai_api_key', '')
            
            if not api_key:
                api_key = os.getenv('OPENAI_API_KEY')
                
            if not api_key:
                print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None
            
            client = openai.OpenAI(api_key=api_key)
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            
            return np.array(response.data[0].embedding, dtype=np.float32)
            
        except Exception as e:
            print(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _search_similar_comments(self, query_vector: np.ndarray, top_k: int) -> List[Dict]:
        """é¡ä¼¼ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
        
        try:
            with sqlite3.connect(self.vector_db_path) as vector_conn:
                cursor = vector_conn.cursor()
                cursor.execute("""
                    SELECT cv.comment_id, cv.user_id, cv.comment_text, cv.vector_data, cv.broadcast_id
                    FROM comment_vectors cv
                """)
                
                results = []
                for row in cursor.fetchall():
                    comment_id, user_id, comment_text, vector_blob, broadcast_id = row
                    
                    # ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒ
                    stored_vector = np.frombuffer(vector_blob, dtype=np.float32)
                    
                    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                    similarity = self._cosine_similarity(query_vector, stored_vector)
                    
                    results.append({
                        'comment_id': comment_id,
                        'user_id': user_id,
                        'comment_text': comment_text,
                        'broadcast_id': broadcast_id,
                        'similarity': similarity,
                        'type': 'comment'
                    })
            
            # é¡ä¼¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            results.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = results[:top_k]
            
            # è¿½åŠ æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åã€æ”¾é€ã‚¿ã‚¤ãƒˆãƒ«ãªã©ï¼‰
            enriched_results = self._enrich_comment_results(top_results)
            
            print(f"ğŸ’¬ é¡ä¼¼ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢å®Œäº†: {len(enriched_results)}ä»¶")
            return enriched_results
            
        except Exception as e:
            print(f"âŒ ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _search_similar_analyses(self, query_vector: np.ndarray, top_k: int) -> List[Dict]:
        """é¡ä¼¼AIåˆ†æã‚’æ¤œç´¢"""
        
        try:
            with sqlite3.connect(self.vector_db_path) as vector_conn:
                cursor = vector_conn.cursor()
                cursor.execute("""
                    SELECT av.analysis_id, av.user_id, av.analysis_text, av.vector_data, av.broadcast_id
                    FROM analysis_vectors av
                """)
                
                results = []
                for row in cursor.fetchall():
                    analysis_id, user_id, analysis_text, vector_blob, broadcast_id = row
                    
                    # ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒ
                    stored_vector = np.frombuffer(vector_blob, dtype=np.float32)
                    
                    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                    similarity = self._cosine_similarity(query_vector, stored_vector)
                    
                    results.append({
                        'analysis_id': analysis_id,
                        'user_id': user_id,
                        'analysis_text': analysis_text,
                        'broadcast_id': broadcast_id,
                        'similarity': similarity,
                        'type': 'analysis'
                    })
            
            # é¡ä¼¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            results.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = results[:top_k]
            
            # è¿½åŠ æƒ…å ±ã‚’å–å¾—
            enriched_results = self._enrich_analysis_results(top_results)
            
            print(f"ğŸ¤– é¡ä¼¼AIåˆ†ææ¤œç´¢å®Œäº†: {len(enriched_results)}ä»¶")
            return enriched_results
            
        except Exception as e:
            print(f"âŒ AIåˆ†ææ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _enrich_comment_results(self, results: List[Dict]) -> List[Dict]:
        """ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢çµæœã«è¿½åŠ æƒ…å ±ã‚’ä»˜ä¸"""
        
        if not results:
            return results
        
        # comment_idã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        comment_ids = [r['comment_id'] for r in results]
        placeholders = ','.join(['?' for _ in comment_ids])
        
        with sqlite3.connect(self.main_db_path) as main_conn:
            cursor = main_conn.cursor()
            cursor.execute(f"""
                SELECT c.id, c.user_name, c.timestamp, c.elapsed_time,
                       b.lv_value, b.live_title, b.start_time,
                       su.display_name
                FROM comments c
                JOIN broadcasts b ON c.broadcast_id = b.id
                LEFT JOIN special_users su ON c.user_id = su.user_id
                WHERE c.id IN ({placeholders})
            """, comment_ids)
            
            # comment_id ã‚’ã‚­ãƒ¼ã¨ã—ãŸè¾æ›¸ã‚’ä½œæˆ
            enrichment_data = {}
            for row in cursor.fetchall():
                comment_id, user_name, timestamp, elapsed_time, lv_value, live_title, start_time, display_name = row
                enrichment_data[comment_id] = {
                    'user_name': user_name,
                    'display_name': display_name or user_name,
                    'timestamp': timestamp,
                    'elapsed_time': elapsed_time,
                    'lv_value': lv_value,
                    'live_title': live_title,
                    'start_time': start_time
                }
        
        # çµæœã«è¿½åŠ æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
        for result in results:
            comment_id = result['comment_id']
            if comment_id in enrichment_data:
                result.update(enrichment_data[comment_id])
                print(f"ğŸ§ª comment_id={comment_id}, display_name={result.get('display_name')}, user_name={result.get('user_name')}")

        
        return results
    
    def _enrich_analysis_results(self, results: List[Dict]) -> List[Dict]:
        """AIåˆ†ææ¤œç´¢çµæœã«è¿½åŠ æƒ…å ±ã‚’ä»˜ä¸"""
        
        if not results:
            return results
        
        analysis_ids = [r['analysis_id'] for r in results]
        placeholders = ','.join(['?' for _ in analysis_ids])
        
        with sqlite3.connect(self.main_db_path) as main_conn:
            cursor = main_conn.cursor()
            cursor.execute(f"""
                SELECT a.id, a.model_used, a.comment_count, a.analysis_date,
                       b.lv_value, b.live_title, b.start_time,
                       su.display_name
                FROM ai_analyses a
                JOIN broadcasts b ON a.broadcast_id = b.id
                LEFT JOIN special_users su ON a.user_id = su.user_id
                WHERE a.id IN ({placeholders})
            """, analysis_ids)
            
            enrichment_data = {}
            for row in cursor.fetchall():
                analysis_id, model_used, comment_count, analysis_date, lv_value, live_title, start_time, display_name = row
                enrichment_data[analysis_id] = {
                    'model_used': model_used,
                    'comment_count': comment_count,
                    'analysis_date': analysis_date,
                    'lv_value': lv_value,
                    'live_title': live_title,
                    'start_time': start_time,
                    'display_name': display_name
                }
        
        for result in results:
            analysis_id = result['analysis_id']
            if analysis_id in enrichment_data:
                result.update(enrichment_data[analysis_id])
        
        return results
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm_a = np.linalg.norm(vec1)
            norm_b = np.linalg.norm(vec2)
            
            if norm_a == 0 or norm_b == 0:
                return 0.0
                
            return float(dot_product / (norm_a * norm_b))
            
        except Exception as e:
            print(f"âŒ é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return 0.0
    
    def _build_context(self, comments: List[Dict], analyses: List[Dict]) -> str:
        """æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""

        context_parts = []

        # ã‚³ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’è¿½åŠ 
        if comments:
            context_parts.append("ã€é–¢é€£ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã€‘")
            for i, comment in enumerate(comments, 1):
                display_name_raw = comment.get('display_name') or ''
                display_name = display_name_raw.strip()
                user_name = (comment.get('user_name') or 'ä¸æ˜').strip()

                # âœ… ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼123456ã€ãªã‚‰ user_name ã‚’å„ªå…ˆ
                if display_name.startswith("ãƒ¦ãƒ¼ã‚¶ãƒ¼") and display_name.replace("ãƒ¦ãƒ¼ã‚¶ãƒ¼", "").isdigit():
                    final_name = user_name
                else:
                    final_name = display_name or user_name

                # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                print(f"ğŸ§ª contextç”¨: user_id={comment.get('user_id')}, raw_display={display_name_raw}, æ¡ç”¨={final_name}")

                user_id = comment.get('user_id', 'ä¸æ˜')
                live_title = comment.get('live_title', 'ä¸æ˜ãªé…ä¿¡')
                elapsed_time = comment.get('elapsed_time', 'ä¸æ˜')
                similarity = comment.get('similarity', 0)

                context_parts.append(
                    f"{i}. ãƒ¦ãƒ¼ã‚¶ãƒ¼: {final_name} (ID: {user_id})"
                    f"\n   ã‚³ãƒ¡ãƒ³ãƒˆ: ã€Œ{comment['comment_text']}ã€"
                    f"\n   é…ä¿¡: {live_title} ({elapsed_time}) [é¡ä¼¼åº¦: {similarity:.3f}]"
                )

        # AIåˆ†ææƒ…å ±ã‚’è¿½åŠ 
        if analyses:
            context_parts.append("\nã€é–¢é€£ã™ã‚‹AIåˆ†æã€‘")
            for i, analysis in enumerate(analyses, 1):
                display_name = analysis.get('display_name', '')
                user_name = analysis.get('user_name', '')
                if display_name.startswith("ãƒ¦ãƒ¼ã‚¶ãƒ¼") and display_name.replace("ãƒ¦ãƒ¼ã‚¶ãƒ¼", "").isdigit():
                    final_name = user_name or display_name
                else:
                    final_name = display_name or user_name

                live_title = analysis.get('live_title', 'ä¸æ˜ãªé…ä¿¡')
                model_used = analysis.get('model_used', 'ä¸æ˜')
                similarity = analysis.get('similarity', 0)
                analysis_preview = analysis['analysis_text'][:150] + "..."

                context_parts.append(
                    f"{i}. ãƒ¦ãƒ¼ã‚¶ãƒ¼: {final_name} (ID: {analysis['user_id']}) ã®åˆ†æ"
                    f"\n   ãƒ¢ãƒ‡ãƒ«: {model_used}"
                    f"\n   è¦ç´„: {analysis_preview}"
                    f"\n   é…ä¿¡: {live_title} [é¡ä¼¼åº¦: {similarity:.3f}]"
                )

        return "\n\n".join(context_parts)




    def _generate_answer(self, question: str, context: str) -> str:
        """LLMã§å›ç­”ç”Ÿæˆ"""
        try:
            # âœ… APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿
            config_path = os.path.join(os.path.dirname(__file__), "config", "ncv_special_config.json")
            api_key = None
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                api_key = config.get('api_settings', {}).get('openai_api_key', '')

            if not api_key:
                api_key = os.getenv('OPENAI_API_KEY')

            if not api_key:
                print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return "âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"

            print(f"ğŸ”‘ ä½¿ç”¨APIã‚­ãƒ¼: {api_key[:8]}...{api_key[-4:]} (é•·ã•: {len(api_key)})")
            client = openai.OpenAI(api_key=api_key, timeout=20.0)

            # âœ… system_prompt ã‚’å¼·åŒ–
            system_prompt = """ã‚ãªãŸã¯ãƒ‹ã‚³ãƒ‹ã‚³ç”Ÿæ”¾é€ã®åˆ†æå°‚é–€å®¶ã§ã™ã€‚
æä¾›ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ï¼ˆç‰¹ã« user_idï¼‰ã‚’å…ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

é‡è¦:
- å›ç­”ã«ã¯å¿…ãšãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰ï¼ˆdisplay_name ã¾ãŸã¯ user_nameï¼‰ã¨ user_id ã‚’å«ã‚ã‚‹
- ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼â—¯â—¯ã€ã®ã‚ˆã†ãªæ±ç”¨åã¯é¿ã‘ã€å®Ÿéš›ã® user_name ã‚’å„ªå…ˆ
- ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã‚’å¼•ç”¨ã—ã¦ã€ãªãœãã†åˆ¤æ–­ã—ãŸã®ã‹ã‚’èª¬æ˜ã™ã‚‹
- å¿…è¦ãªã‚‰ user_id ã‚’ä½¿ã£ã¦äººç‰©ã‚’æ˜ç¢ºåŒ–ã™ã‚‹
- ä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ã¯ã€Œä¸è¶³ã—ã¦ã„ã‚‹ã€ã¨æ˜ç¤º
- ç°¡æ½”ã‹ã¤å…·ä½“çš„ã«ç­”ãˆã‚‹
"""
            user_prompt = f"""è³ªå•: {question}

    å‚è€ƒæƒ…å ±:
    {context}

    ã“ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€è³ªå•ã«å¯¾ã™ã‚‹æ˜ç¢ºã§å…·ä½“çš„ãªç­”ãˆã‚’å‡ºã—ã¦ãã ã•ã„ã€‚
    """

            # âœ… å®Ÿéš›ã«é€ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸¸ã”ã¨è¡¨ç¤º
            print("\nğŸ“ --- LLMã¸é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---")
            print(f"[SYSTEM PROMPT]\n{system_prompt}\n")
            print(f"[USER PROMPT]\n{user_prompt}\n")
            print("ğŸ“ --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã“ã“ã¾ã§ ---\n")

            print("ğŸ¤– AIå›ç­”ç”Ÿæˆä¸­... (gpt-4o ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡)")
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=800,
                temperature=0.4
            )
            print("âœ… OpenAI APIå‘¼ã³å‡ºã—å®Œäº†")

            answer = response.choices[0].message.content.strip()
            print("âœ… AIå›ç­”ç”Ÿæˆå®Œäº†")
            return answer

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ã‚’å‡ºåŠ›ã—ã¦åŸå› ã‚’ç‰¹å®šã—ã‚„ã™ãã™ã‚‹
            print("âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            print("âŒ é€ä¿¡ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
            print(f"[SYSTEM PROMPT]\n{system_prompt}\n")
            print(f"[USER PROMPT]\n{user_prompt}\n")
            return f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"



    
    def get_system_status(self) -> Dict:
        """RAGã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ³ã‚’å–å¾—"""
        
        status = {
            'vector_db_exists': os.path.exists(self.vector_db_path),
            'main_db_exists': os.path.exists(self.main_db_path),
            'total_comment_vectors': 0,
            'total_analysis_vectors': 0,
            'unique_broadcasts': 0,
            'unique_users': 0
        }
        
        if status['vector_db_exists']:
            with sqlite3.connect(self.vector_db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute("SELECT COUNT(*) FROM comment_vectors")
                status['total_comment_vectors'] = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM analysis_vectors")
                status['total_analysis_vectors'] = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(DISTINCT broadcast_id) FROM comment_vectors")
                status['unique_broadcasts'] = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(DISTINCT user_id) FROM comment_vectors")
                status['unique_users'] = cursor.fetchone()[0]
        
        return status

# ä½¿ç”¨ä¾‹ãƒ»ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    import sys

    rag = RAGSearchSystem()

    # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ç¢ºèª
    status = rag.get_system_status()
    print("ğŸ” RAGã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³:")
    for key, value in status.items():
        print(f"  {key}: {value}")

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒã‚ã‚Œã°ãã‚Œã‚’è³ªå•ã«ä½¿ã†
    if len(sys.argv) > 1:
        question = " ".join(sys.argv[1:])
    else:
        question = "é¢ç™½ã„ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã—ãŸäººã¯èª°ã§ã™ã‹ï¼Ÿ"

    if status['total_comment_vectors'] > 0:
        print(f"\nğŸ“ è³ªå•: {question}")
        answer = rag.search_and_answer(question)
        print(f"\nğŸ’¡ å›ç­”:\n{answer}")
